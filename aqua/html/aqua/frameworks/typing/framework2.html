<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>aqua.frameworks.typing.framework2 API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>aqua.frameworks.typing.framework2</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import pdb
import cv2
import aqua
import math
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from matplotlib import pyplot as plt
from torch.utils.data import DataLoader
from aqua.nn.dloaders import AOLMETrmsDLoader
import pytkit as pk
from aqua.nn.models import DyadicCNN3D
from torchsummary import summary


class Typing:
    tydf = pd.DataFrame()
    &#34;&#34;&#34; Data frame having information on typing instances. 
    It has following columns,
    ```
    1. f0     : poc of starting frame
    2. f      : number of frames
    3. W, H   : Video width and height
    4. w0, h0 : Bounding box top left corner
    5. w, h   : width and height of bounding box
    6. FPS    : Frames per second
    7. typing : {-1, 0, 1}.
                -1 =&gt; Keyboard not found
                 0 =&gt; notyping
                 1 =&gt; typing
    ```
    &#34;&#34;&#34;

    cfg = {}
    &#34;&#34;&#34; Configuration dictionary &#34;&#34;&#34;

    ivid = None
    &#34;&#34;&#34; Input video instance. &#34;&#34;&#34;

    tydur = 0
    &#34;&#34;&#34; Duration of spati-temporal trims to classify. &#34;&#34;&#34;
    
    
    
    def __init__(self, cfg, tydur=3):
        &#34;&#34;&#34; Spatio temporal typing detection using,

        Parameters
        ----------
        cfg : Str
        Configuration file. The configuration has the following entries.
        
            1. &#34;video&#34;: Video path for which we have keyboard detections.
            2. &#34;keyboard&#34;: Keyboard detections as CSV file
            3. &#34;table_roi&#34;: CSV file having Table ROI marked
            4. &#34;depth&#34;: Depth of 3D-CNN that provided the best performance,
            5. &#34;ckpt&#34;: Checkpoint of 3D-CNN
        
        &#34;&#34;&#34;

        # Configuration dictionary
        self.cfg = cfg
        
        # tydur
        self.tydur = tydur
        
        # Checking configuration file
        self._check_cfg(cfg)

        # Loading video
        self.ivid = pk.Vid(cfg[&#39;video&#39;], &#39;read&#39;)
        
        
    def get_typing_proposals(self):
        &#34;&#34;&#34; Generates typing proposal regions by using keyboard detections
        and manually annotated table region of interest.
        &#34;&#34;&#34;
        
        # Creating an empty typing region proposal dataframe
        tyrp = pd.DataFrame()

        # Load keyboard and table roi csvs
        keyboard_df = pd.read_csv(self.cfg[&#39;keyboard&#39;])
        troi_df = pd.read_csv(self.cfg[&#39;table_roi&#39;])
        troi_df = troi_df[troi_df[&#39;video_names&#39;] == f&#34;{self.ivid.props[&#39;name&#39;]}{self.ivid.props[&#39;extension&#39;]}&#34;].copy()
        
        # Calcuating typing region proposals every 3 seconds using keyboard detections
        fps = keyboard_df[&#39;FPS&#39;].unique().item()
        new_rows = []
        for i in tqdm(range(0, troi_df[&#39;f0&#39;].max(), 3*fps), desc=&#34;INFO: typing proposals&#34;):

            # Get keyboards and table regions for 3 seconds
            kdf = keyboard_df[keyboard_df[&#39;f0&#39;].between(i, i + 3*fps - 1)].copy()
            tdf = troi_df[troi_df[&#39;f0&#39;].between(i, i + 3*fps - 1)].copy()

            # Skip,
            # 1. if the current 3 seconds do not have sufficient table regions of interest
            # 2. if the keyboards dataframe is empty
            if (not self._have_sufficient_rois(tdf.copy())) or (len(kdf) == 0):
                continue

            # Calculating table boundary from regions of interest
            table_boundary = self._get_table_boundary(tdf.copy())

            # Calculating overlap ratio between table bondary and keyboard detections
            kdf = self._get_roi_overlap_ratio(kdf, table_boundary)

            # Remove keyboard regions that do not corss a overlap ratio threshold
            kdf = self._remove_outside_keyboard_detections(kdf.copy())

            # Skip to next 3 seconds if all the keyboard regions lie outside
            # the table boundary
            if len(kdf) == 0:
                continue

            # Calculate keyboard region proposal rows for 3 seconds by tabking union
            # of all the valid keyboard detections
            new_rows += self._get_union_of_keyboard_detections(kdf.copy(), i, 3*fps)


        tyrp = pd.DataFrame(new_rows, columns=[
                &#39;W&#39;, &#39;H&#39;, &#39;FPS&#39;, &#39;f0&#39;, &#39;f&#39;, &#39;class&#39;, &#39;table_boundary&#39;,
                &#39;w0&#39;, &#39;h0&#39;, &#39;w&#39;, &#39;h&#39;
            ])
            
        return tyrp

    def classify_typing_proposals(self, trp, debug=True):
        &#34;&#34;&#34; Classify each proposed region as typing / no-typing.

        Todo
        ----
        This function evaluates one proposal at a time. This is note
        optimal. I have to redo this to evaluate multiple proposals
        at a time. 

        Parameters
        ----------
        trp : Pandas DataFrame instance
            DataFrame having typing region proposals
        debug : Bool
            When True we will save the class probability and trimmed
            videos.

        Return
        ------
        A DataFrame with `typing` column with labels 1 and 0 for
        typing and no-typing respectively.
        &#34;&#34;&#34;
        
        # Output location to save trims and typing csv file
        oloc = self.ivid.props[&#39;dir_loc&#39;]
        
        # Creating another column in typing region proposal dataframe
        trp[&#39;typing&#39;] = -1
        if debug:
            print(f&#34;INFO: RUNNING IN DEBUG MODE!&#34;)
            trp[&#39;p&#39;] = -1
            trp[&#39;trim_path&#39;] = &#34;&#34;
            os.system(f&#34;rm -r {oloc}/trims&#34;)
            os.system(f&#34;mkdir -p {oloc}/trims/0&#34;)
            os.system(f&#34;mkdir -p {oloc}/trims/1&#34;)
        
        # Loading neural network into GPU
        net = self._load_net(self.cfg)

        # Loop through typing proposal
        for i, row in tqdm(trp.iterrows(), total=trp.shape[0], desc=&#34;INFO: Classifying&#34;):

            # Spatio temporal trim coordinates
            bbox = [row[&#39;w0&#39;],row[&#39;h0&#39;], row[&#39;w&#39;], row[&#39;h&#39;]]
            sfrm = row[&#39;f0&#39;]
            efrm = sfrm + row[&#39;f&#39;]
            opth = (f&#34;{oloc}/temp.mp4&#34;)

            # Spatio temporal trim
            self.ivid.save_spatiotemporal_trim(sfrm, efrm, bbox, opth)

            # Creating a temporary text file `temp.txt` having
            # temp.mp4 and a dummy label (100)
            with open(f&#34;{oloc}/temp.txt&#34;, &#34;w&#34;) as f:
                f.write(&#34;temp.mp4 100&#34;)

                    

            # Intialize AOLME data loader instance
            tst_data = AOLMETrmsDLoader(
                oloc, f&#34;{oloc}/temp.txt&#34;, oshape=(224, 224)
            )
            tst_loader = DataLoader(
                tst_data, batch_size=1, num_workers=1
            )

            # Loop over tst data (??? goes over only once. I am desparate so I kept the loop)
            for data in tst_loader:
                dummy_labels, inputs = (
                    data[0].to(&#34;cuda:0&#34;, non_blocking=True),
                    data[1].to(&#34;cuda:0&#34;, non_blocking=True)
                )
                with torch.no_grad():
                    try:
                        outputs = net(inputs)
                    except:
                        import pdb; pdb.set_trace()
                    ipred = outputs.data.clone()
                    ipred = ipred.to(&#34;cpu&#34;).numpy().flatten().tolist()
                    trp.at[i, &#39;typing&#39;] = round(ipred[0])
                    if debug:
                        trim_pth = f&#34;{oloc}/trims/{round(ipred[0])}/trim_{i}.mp4&#34;
                        os.system(f&#34;cp {oloc}/temp.mp4 {trim_pth}&#34;)
                        trp.at[i, &#39;p&#39;] = ipred[0]
                        trp.at[i, &#39;trim_path&#39;] = f&#34;{round(ipred[0])}/trim_{i}.mp4&#34;
        return trp

                    
    def _get_union_of_keyboard_detections(self, df, f0, f):
        &#34;&#34;&#34; Returns keyboard detection regions using union of all the detections
        in an interval.
        
        Parameters
        ----------
        df : DataFrame
            A DataFrame having keyboard detections.

        Returns
        -------
        Returns list of new rows with following columns
        [W, H, FPS, f0, f, class, table_boundary, w0, h0, w, h]
        &#34;&#34;&#34;
        # Flag to turn off/on images
        show_images = False
        
        # Creating an image with zeros
        W = df[&#39;W&#39;].unique().item()
        H = df[&#39;H&#39;].unique().item()
        FPS = df[&#39;FPS&#39;].unique().item()
        oclass = &#34;keyboard&#34;
        table_boundary = df[&#39;table_boundary&#39;].unique().item()
        uimg = np.zeros((H, W ))

        # Creating a binary image that is union of all the bounding boxes.
        for i, r in df.iterrows():
            [w0, h0, w, h] = [r[&#39;w0&#39;], r[&#39;h0&#39;], r[&#39;w&#39;], r[&#39;h&#39;]]
            uimg[h0 : h0 + h, w0 : w0 + w] = 1

        if show_images:
            plt.subplot(111)
            ax = plt.subplot(1, 1, 1)
            ax.imshow(uimg, cmap=&#39;gray&#39;)
            plt.show()

        # Connected components
        cc = cv2.connectedComponentsWithStats(uimg.astype(&#39;uint8&#39;), 4, cv2.CV_32S)
        cc_img = cc[1]
        cc_labels = np.unique(cc_img).tolist()

        # Loop through each label and find bounding box coordinates
        new_rows = []
        for cc_label in cc_labels[1:]:
            
            new_row = [W, H, FPS, f0, f, oclass, table_boundary]

            cc_label_img = 1*(cc_img == cc_label).astype(&#39;uint8&#39;)
            active_px = np.argwhere(cc_label_img!=0)
            active_px = active_px[:,[1,0]]
            w0,h0,w,h = cv2.boundingRect(active_px)

            new_row += [w0, h0, w, h]

            new_rows += [new_row]

        return new_rows

    
    def _remove_outside_keyboard_detections(self, df, th=0.5):
        &#34;&#34;&#34; Removes all the keyboard detections that are less that are
        50% not inside the table boundary.

        Parameters
        ----------
        df : DataFrame
            DataFrame having keyboard detections with `roi-overlap-ratio`
            column.
        th : Float
            Detectons which are &lt; th are removed.
        &#34;&#34;&#34;
        for ridx, row in df.iterrows():
            if row[&#39;roi-overlap-ratio&#39;] &lt; th:
                df.drop([ridx], inplace = True)
        return df

    
    def _get_roi_overlap_ratio(self, hdf, table_boundary):
        &#34;&#34;&#34; Adds a column to keyboard detections, roi-overlap-ratio.
            
            - Table boundary = T
            - Keyboard detection = H
            - Overlap (O) = Intersection(T, H)
                overlap-ratio = Area(O) / Area(H)

        Parameters
        ----------
        hdf : DataFrame
            Dataframe having keyboard detections

        table_boundary : List[Int]
            Table boundary as list, [w0, h0, w, h]
        &#34;&#34;&#34;
        # Flag to turn off/on images
        show_images = False
        
        # Uncompressing boundary
        [tw0, th0, tw, th] = table_boundary
        
        # Creating an image with zeros
        W = hdf[&#39;W&#39;].unique().item()
        H = hdf[&#39;H&#39;].unique().item()
        zimg = np.zeros((H,W))

        # Creating a binary image with table boundary marked as 1s
        timg = zimg.copy()
        timg[th0 : th0 + th, tw0 : tw0 + tw] = 1

        # Loop over each keyboard detection
        o_area_ratio_lst = []
        for ridx, row in hdf.iterrows():

            # keyboard detection is loaded into proper variables
            [hw0, hh0, hw, hh] = [row[&#39;w0&#39;], row[&#39;h0&#39;], row[&#39;w&#39;], row[&#39;h&#39;]]
            h_area = hw*hh

            # Creating a binary image with keyboard detection as 1
            himg = zimg.copy()
            himg[hh0 : hh0 + hh, hw0 : hw0 + hw] = 1

            # Overlap image
            oimg = himg + timg
            oimg = 1*(oimg == 2)
            o_area = oimg.sum()

            if show_images:
                plt.subplot(221)
                ax1 = plt.subplot(2, 2, 1)
                ax2 = plt.subplot(2, 2, 2)
                ax3 = plt.subplot(2, 2, 3)
                ax1.imshow(timg)
                ax2.imshow(himg)
                ax3.imshow(oimg)
                plt.show()
                import pdb; pdb.set_trace()

            # Drop the keyboard detection if the overlap area is less than
            # 50% of keyboard area
            o_area_ratio = o_area/h_area
            o_area_ratio_lst += [o_area_ratio]
            
        hdf[&#39;table_boundary&#39;] = f&#34;{tw0}-{th0}-{tw}-{th}&#34;
        hdf[&#39;roi-overlap-ratio&#39;] = o_area_ratio_lst

        return hdf

    
    def _have_sufficient_rois(self, df):
        &#34;&#34;&#34; Returns true if there there is a vlaid region of interest of atleast one
        student. A student having atleast tow rois out of three is considered valid.

        Parameters
        ----------
        df : DataFrame
            A data frame having roi entries for `self.tydur`.
        &#34;&#34;&#34;

        # Dropping unnecessary columns
        df.drop([&#39;Time&#39;, &#39;f0&#39;, &#39;video_names&#39;, &#39;f&#39;], axis = 1, inplace=True)

        # Looping over each column and if atleast one column contain 2 valid entries
        # return true
        for col in df.columns.tolist():
            
            valid_bboxes = 0
            for i in range(0, self.tydur):

                # Bounding box in current column
                bbox_coords = df[col].iloc[i]

                # A bounding box is valid if it has four coordinates
                # separated by &#34;-&#34;
                if len(bbox_coords.split(&#34;-&#34;)) == 4:
                    valid_bboxes += 1

                # If we are able to get more than 2 valid bounding
                # boxes return True
                if valid_bboxes &gt;= 2:
                    return True

        # If thre are no valid bounding boxes return False
        return False

    
    def _get_table_boundary(self, df):
        &#34;&#34;&#34; Return table boundary as list, [w0, h0, w, h]

        Parameters
        ----------
        df : DataFrame
            A data frame having roi entries for `self.tydur`.
        &#34;&#34;&#34;

        # Dropping unnecessary columns
        df.drop([&#39;Time&#39;, &#39;f0&#39;, &#39;video_names&#39;, &#39;f&#39;], axis = 1, inplace=True)

        # Creating list of bounding box coordinates
        w_min = math.inf
        h_min = math.inf
        w_max = 0
        h_max = 0
        for col in df.columns.tolist():
            for ridx in range(0, len(df[col])):
                
                [w0, h0, w, h] = [int(x) for x in df[col].iloc[ridx].split(&#39;-&#39;)]

                if w_min &gt; w0:
                    w_min = w0
                if h_min &gt; h0:
                    h_min = h0
                if w_max &lt; w0 + w:
                    w_max = w0 + w
                if h_max &lt; h0 + h:
                    h_max = h0 + h

        boundary = [w_min, h_min, w_max - w_min, h_max - h_min]

        return boundary
        

    
    def _check_cfg(self, cfg):
        &#34;&#34;&#34; Validates the configuration file.&#34;&#34;&#34;

        # Check files
        pk.check_file_existance(cfg[&#39;video&#39;])
        pk.check_file_existance(cfg[&#39;keyboard&#39;])
        pk.check_file_existance(cfg[&#39;table_roi&#39;])
        pk.check_file_existance(cfg[&#39;ckpt&#39;])
        
        # Depth should be between 2 and 4
        if cfg[&#39;depth&#39;] &lt; 2 or cfg[&#39;depth&#39;] &gt; 4:
            raise Exception(
                &#34;USER EXCEPTION: The dyadic depth is not valid.&#34;
                f&#34;\n\t Depth: {cfg[&#39;depth&#39;]}&#34;
            )


        
    def _load_net(self, cfg):
        &#34;&#34;&#34; Load neural network to GPU. &#34;&#34;&#34;

        print(&#34;INFO: Loading Trained network to GPU ...&#34;)

        # Checkpoint and depth from cfg file.
        ckpt = cfg[&#39;ckpt&#39;]
        depth = cfg[&#39;depth&#39;]

        # Creating an instance of Dyadic 3D-CNN
        net = DyadicCNN3D(depth, [3, 90, 224, 224])
        net.to(&#34;cuda:0&#34;)

        # Print summary of network.
        # summary(net, (3, 90, 224, 224))

        # Loading the net with trained weights to cuda device 0
        ckpt_weights = torch.load(ckpt)
        net.load_state_dict(ckpt_weights[&#39;model_state_dict&#39;])
        net.eval()

        return net
        
        

    def _check_for_typing(self, proposal_df):
        &#34;&#34;&#34; Checks for typing in the proposal data frame.

        Parameters
        ----------
        proposal_df: DataFrame
            Proposal dataframe having keyboards bounding boxes.
        Todo
        ----
        1. Here I am trimming -&gt; typing to hdd -&gt; loading. This is not
           recommended for speed. Please try to improve.
        &#34;&#34;&#34;
        import pdb; pdb.set_trace()
        # Loop over proposal dataframe
        for i, row in proposal_df.iterrows():

            # if w or h == 0 then there is no keyboards
            if row[&#39;w&#39;] == 0 or row[&#39;h&#39;] == 0:
                proposal_df.at[i, &#39;typing&#39;] = -1
            else:
                # Creating temporal trim
                bbox = [row[&#39;w0&#39;],row[&#39;h0&#39;], row[&#39;w&#39;], row[&#39;h&#39;]]
                sfrm = row[&#39;f0&#39;]
                efrm = sfrm + row[&#39;f&#39;]
                oloc = f&#34;{os.path.dirname(self._vid.props[&#39;dir_loc&#39;])}&#34;
                opth = (f&#34;{oloc}/temp.mp4&#34;)

                # Spatio temporal trim
                self._vid.spatiotemporal_trim(sfrm, efrm, bbox, opth)
                 
                # Creating a temporary text file `temp.txt` having
                # temp.mp4 and a dummy label (100)
                with open(f&#34;{oloc}/temp.txt&#34;, &#34;w&#34;) as f:
                    f.write(&#34;temp.mp4 100&#34;)

                # Intialize AOLME data loader instance
                tst_data = AOLMETrmsDLoader(oloc, f&#34;{oloc}/temp.txt&#34;, oshape=(224, 224))
                tst_loader = DataLoader(tst_data, batch_size=1, num_workers=1)

                # Loop over tst data (goes over only once. I am desparate so I kept the loop)
                for data in tst_loader:
                    dummy_labels, inputs = (data[0].to(&#34;cuda:0&#34;, non_blocking=True),
                                             data[1].to(&#34;cuda:0&#34;, non_blocking=True))
                    with torch.no_grad():
                        outputs = self._net(inputs)
                        ipred = outputs.data.clone()
                        ipred = ipred.to(&#34;cpu&#34;).numpy().flatten().tolist()
                        proposal_df.at[i, &#39;typing&#39;] = round(ipred[0])
                        
        return proposal_df



    def _get_proposal_df(self, bboxes, tydur):
        &#34;&#34;&#34;
        OBSOLETE SHOULD BE DELETED IN CLEANUP PHASE.
        Creates a data frame with each row representing 3 seconds.

        Parameters
        ----------
        bboxes: str
            path to file having keyboards bounding boxes
        tydur: int, optional
            Each typing instance duration considered in seconds. 
            Defaults to 3.
        &#34;&#34;&#34;
        # Video properties
        num_frames = self._vid.props[&#39;num_frames&#39;]
        fps = self._vid.props[&#39;frame_rate&#39;]

        # Creating f0 and f lists
        num_trims = math.floor(num_frames/(tydur*fps))
        f0 = [x*(tydur*fps) for x in range(0, num_trims)]
        f = [tydur*fps]*num_trims

        # Creating W, H and FPS lists
        W = [self._vid.props[&#39;width&#39;]]*num_trims
        H = [self._vid.props[&#39;height&#39;]]*num_trims
        fps_lst = [fps]*num_trims

        # Get bounding boxes
        w0, h0, w, h = self._get_proposal_bboxes(bboxes, f0, f)

        # Intializing all typing instances are marked nan(numpy)
        typing_lst = [np.nan]*(num_trims)

        # Creating data frame with all the lists
        df = pd.DataFrame(list(zip(W, H, fps_lst, f0, f, w0, h0, w, h, typing_lst)),
                          columns=[&#34;W&#34;,&#34;H&#34;, &#34;FPS&#34;, &#34;f0&#34;, &#34;f&#34;, &#34;w0&#34;, &#34;h0&#34;, &#34;w&#34;, &#34;h&#34;, &#34;typing&#34;])
        return df


    
    def write_to_csv(self):
        &#34;&#34;&#34; Writes typing instances to a csv file. The name of the file is `&lt;video name&gt;_wr_using_alg.csv` and has
        following columns,

            1. f0      : poc of starting frame
            2. f       : number of frames
            3. W, H    : Video width and height
            4. w0, h0  : Bounding box top left corner
            5. w, h    : width and height of bounding box
            6. FPS     : Frames per second
            7. typing : {-1, 0, 1}.
                -1 =&gt; Keyboards not found
                0  =&gt; notyping
                1  =&gt; typing

        &#34;&#34;&#34;
        # Update typing instances in typing dataframe by processing
        # valid instances to 0 or 1
        self.tydf = self._check_for_typing(self._proposal_df.copy())
        
        vname = self._vid.props[&#39;name&#39;]
        vloc = self._vid.props[&#39;dir_loc&#39;]
        csv_pth = f&#34;{vloc}/{vname}_wr_using_alg.csv&#34;
        self.tydf.to_csv(csv_pth)
        

        
    def _get_proposal_bboxes(self, bboxes, f0_lst, f_lst):
        &#34;&#34;&#34; Creates proposal bounding boxes. Trims from these bounding boxes are
        later processed via typing detection algorithm.

        If there are multiple bounding boxes in the duration we consider the
        union of bounding boxes.

        Parameters
        ----------
        bboxes: str
            Path to file having keyboards detection bounding boxes
        f0_lst: List of int
            List having starting frame poc
        f_lst: List of int
            List having poc lenght
        &#34;&#34;&#34;
        df_bb = pd.read_csv(bboxes)
        num_trims = len(f0_lst)
        
        w0_lst = [0]*num_trims
        h0_lst = [0]*num_trims
        w_lst = [0]*num_trims
        h_lst = [0]*num_trims
        
        for i in range(0, num_trims):
            f0 = f0_lst[i]
            f = f_lst[i]

            # Data frame having detections from f0 to f0+f
            df_bbi = pd.DataFrame()
            df_bbi = df_bb[df_bb[&#39;f0&#39;] &gt;= f0].copy()
            df_bbi = df_bbi[df_bbi[&#39;f0&#39;] &lt; f0 + f]
            if len(df_bbi) &gt; 0:
                w0i = df_bbi[&#39;w0&#39;].tolist()
                h0i = df_bbi[&#39;h0&#39;].tolist()
                wi = df_bbi[&#39;w&#39;].tolist()
                hi = df_bbi[&#39;h&#39;].tolist()
                w1i = [sum(x) for x in zip(w0i, wi)]
                h1i = [sum(x) for x in zip(h0i, hi)]

                # Taking union
                w0_tl = min(w0i)
                h0_tl = min(h0i)
                w0_br = max(w1i)
                h0_br = max(h1i)

                # Adding to the list
                w0_lst[i] = w0_tl
                h0_lst[i] = h0_tl
                w_lst[i] = w0_br - w0_tl
                h_lst[i] = h0_br - h0_tl
        return (w0_lst, h0_lst, w_lst, h_lst)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="aqua.frameworks.typing.framework2.Typing"><code class="flex name class">
<span>class <span class="ident">Typing</span></span>
<span>(</span><span>cfg, tydur=3)</span>
</code></dt>
<dd>
<div class="desc"><p>Spatio temporal typing detection using,</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>cfg</code></strong> :&ensp;<code>Str</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Configuration file. The configuration has the following entries.</p>
<pre><code>1. "video": Video path for which we have keyboard detections.
2. "keyboard": Keyboard detections as CSV file
3. "table_roi": CSV file having Table ROI marked
4. "depth": Depth of 3D-CNN that provided the best performance,
5. "ckpt": Checkpoint of 3D-CNN
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Typing:
    tydf = pd.DataFrame()
    &#34;&#34;&#34; Data frame having information on typing instances. 
    It has following columns,
    ```
    1. f0     : poc of starting frame
    2. f      : number of frames
    3. W, H   : Video width and height
    4. w0, h0 : Bounding box top left corner
    5. w, h   : width and height of bounding box
    6. FPS    : Frames per second
    7. typing : {-1, 0, 1}.
                -1 =&gt; Keyboard not found
                 0 =&gt; notyping
                 1 =&gt; typing
    ```
    &#34;&#34;&#34;

    cfg = {}
    &#34;&#34;&#34; Configuration dictionary &#34;&#34;&#34;

    ivid = None
    &#34;&#34;&#34; Input video instance. &#34;&#34;&#34;

    tydur = 0
    &#34;&#34;&#34; Duration of spati-temporal trims to classify. &#34;&#34;&#34;
    
    
    
    def __init__(self, cfg, tydur=3):
        &#34;&#34;&#34; Spatio temporal typing detection using,

        Parameters
        ----------
        cfg : Str
        Configuration file. The configuration has the following entries.
        
            1. &#34;video&#34;: Video path for which we have keyboard detections.
            2. &#34;keyboard&#34;: Keyboard detections as CSV file
            3. &#34;table_roi&#34;: CSV file having Table ROI marked
            4. &#34;depth&#34;: Depth of 3D-CNN that provided the best performance,
            5. &#34;ckpt&#34;: Checkpoint of 3D-CNN
        
        &#34;&#34;&#34;

        # Configuration dictionary
        self.cfg = cfg
        
        # tydur
        self.tydur = tydur
        
        # Checking configuration file
        self._check_cfg(cfg)

        # Loading video
        self.ivid = pk.Vid(cfg[&#39;video&#39;], &#39;read&#39;)
        
        
    def get_typing_proposals(self):
        &#34;&#34;&#34; Generates typing proposal regions by using keyboard detections
        and manually annotated table region of interest.
        &#34;&#34;&#34;
        
        # Creating an empty typing region proposal dataframe
        tyrp = pd.DataFrame()

        # Load keyboard and table roi csvs
        keyboard_df = pd.read_csv(self.cfg[&#39;keyboard&#39;])
        troi_df = pd.read_csv(self.cfg[&#39;table_roi&#39;])
        troi_df = troi_df[troi_df[&#39;video_names&#39;] == f&#34;{self.ivid.props[&#39;name&#39;]}{self.ivid.props[&#39;extension&#39;]}&#34;].copy()
        
        # Calcuating typing region proposals every 3 seconds using keyboard detections
        fps = keyboard_df[&#39;FPS&#39;].unique().item()
        new_rows = []
        for i in tqdm(range(0, troi_df[&#39;f0&#39;].max(), 3*fps), desc=&#34;INFO: typing proposals&#34;):

            # Get keyboards and table regions for 3 seconds
            kdf = keyboard_df[keyboard_df[&#39;f0&#39;].between(i, i + 3*fps - 1)].copy()
            tdf = troi_df[troi_df[&#39;f0&#39;].between(i, i + 3*fps - 1)].copy()

            # Skip,
            # 1. if the current 3 seconds do not have sufficient table regions of interest
            # 2. if the keyboards dataframe is empty
            if (not self._have_sufficient_rois(tdf.copy())) or (len(kdf) == 0):
                continue

            # Calculating table boundary from regions of interest
            table_boundary = self._get_table_boundary(tdf.copy())

            # Calculating overlap ratio between table bondary and keyboard detections
            kdf = self._get_roi_overlap_ratio(kdf, table_boundary)

            # Remove keyboard regions that do not corss a overlap ratio threshold
            kdf = self._remove_outside_keyboard_detections(kdf.copy())

            # Skip to next 3 seconds if all the keyboard regions lie outside
            # the table boundary
            if len(kdf) == 0:
                continue

            # Calculate keyboard region proposal rows for 3 seconds by tabking union
            # of all the valid keyboard detections
            new_rows += self._get_union_of_keyboard_detections(kdf.copy(), i, 3*fps)


        tyrp = pd.DataFrame(new_rows, columns=[
                &#39;W&#39;, &#39;H&#39;, &#39;FPS&#39;, &#39;f0&#39;, &#39;f&#39;, &#39;class&#39;, &#39;table_boundary&#39;,
                &#39;w0&#39;, &#39;h0&#39;, &#39;w&#39;, &#39;h&#39;
            ])
            
        return tyrp

    def classify_typing_proposals(self, trp, debug=True):
        &#34;&#34;&#34; Classify each proposed region as typing / no-typing.

        Todo
        ----
        This function evaluates one proposal at a time. This is note
        optimal. I have to redo this to evaluate multiple proposals
        at a time. 

        Parameters
        ----------
        trp : Pandas DataFrame instance
            DataFrame having typing region proposals
        debug : Bool
            When True we will save the class probability and trimmed
            videos.

        Return
        ------
        A DataFrame with `typing` column with labels 1 and 0 for
        typing and no-typing respectively.
        &#34;&#34;&#34;
        
        # Output location to save trims and typing csv file
        oloc = self.ivid.props[&#39;dir_loc&#39;]
        
        # Creating another column in typing region proposal dataframe
        trp[&#39;typing&#39;] = -1
        if debug:
            print(f&#34;INFO: RUNNING IN DEBUG MODE!&#34;)
            trp[&#39;p&#39;] = -1
            trp[&#39;trim_path&#39;] = &#34;&#34;
            os.system(f&#34;rm -r {oloc}/trims&#34;)
            os.system(f&#34;mkdir -p {oloc}/trims/0&#34;)
            os.system(f&#34;mkdir -p {oloc}/trims/1&#34;)
        
        # Loading neural network into GPU
        net = self._load_net(self.cfg)

        # Loop through typing proposal
        for i, row in tqdm(trp.iterrows(), total=trp.shape[0], desc=&#34;INFO: Classifying&#34;):

            # Spatio temporal trim coordinates
            bbox = [row[&#39;w0&#39;],row[&#39;h0&#39;], row[&#39;w&#39;], row[&#39;h&#39;]]
            sfrm = row[&#39;f0&#39;]
            efrm = sfrm + row[&#39;f&#39;]
            opth = (f&#34;{oloc}/temp.mp4&#34;)

            # Spatio temporal trim
            self.ivid.save_spatiotemporal_trim(sfrm, efrm, bbox, opth)

            # Creating a temporary text file `temp.txt` having
            # temp.mp4 and a dummy label (100)
            with open(f&#34;{oloc}/temp.txt&#34;, &#34;w&#34;) as f:
                f.write(&#34;temp.mp4 100&#34;)

                    

            # Intialize AOLME data loader instance
            tst_data = AOLMETrmsDLoader(
                oloc, f&#34;{oloc}/temp.txt&#34;, oshape=(224, 224)
            )
            tst_loader = DataLoader(
                tst_data, batch_size=1, num_workers=1
            )

            # Loop over tst data (??? goes over only once. I am desparate so I kept the loop)
            for data in tst_loader:
                dummy_labels, inputs = (
                    data[0].to(&#34;cuda:0&#34;, non_blocking=True),
                    data[1].to(&#34;cuda:0&#34;, non_blocking=True)
                )
                with torch.no_grad():
                    try:
                        outputs = net(inputs)
                    except:
                        import pdb; pdb.set_trace()
                    ipred = outputs.data.clone()
                    ipred = ipred.to(&#34;cpu&#34;).numpy().flatten().tolist()
                    trp.at[i, &#39;typing&#39;] = round(ipred[0])
                    if debug:
                        trim_pth = f&#34;{oloc}/trims/{round(ipred[0])}/trim_{i}.mp4&#34;
                        os.system(f&#34;cp {oloc}/temp.mp4 {trim_pth}&#34;)
                        trp.at[i, &#39;p&#39;] = ipred[0]
                        trp.at[i, &#39;trim_path&#39;] = f&#34;{round(ipred[0])}/trim_{i}.mp4&#34;
        return trp

                    
    def _get_union_of_keyboard_detections(self, df, f0, f):
        &#34;&#34;&#34; Returns keyboard detection regions using union of all the detections
        in an interval.
        
        Parameters
        ----------
        df : DataFrame
            A DataFrame having keyboard detections.

        Returns
        -------
        Returns list of new rows with following columns
        [W, H, FPS, f0, f, class, table_boundary, w0, h0, w, h]
        &#34;&#34;&#34;
        # Flag to turn off/on images
        show_images = False
        
        # Creating an image with zeros
        W = df[&#39;W&#39;].unique().item()
        H = df[&#39;H&#39;].unique().item()
        FPS = df[&#39;FPS&#39;].unique().item()
        oclass = &#34;keyboard&#34;
        table_boundary = df[&#39;table_boundary&#39;].unique().item()
        uimg = np.zeros((H, W ))

        # Creating a binary image that is union of all the bounding boxes.
        for i, r in df.iterrows():
            [w0, h0, w, h] = [r[&#39;w0&#39;], r[&#39;h0&#39;], r[&#39;w&#39;], r[&#39;h&#39;]]
            uimg[h0 : h0 + h, w0 : w0 + w] = 1

        if show_images:
            plt.subplot(111)
            ax = plt.subplot(1, 1, 1)
            ax.imshow(uimg, cmap=&#39;gray&#39;)
            plt.show()

        # Connected components
        cc = cv2.connectedComponentsWithStats(uimg.astype(&#39;uint8&#39;), 4, cv2.CV_32S)
        cc_img = cc[1]
        cc_labels = np.unique(cc_img).tolist()

        # Loop through each label and find bounding box coordinates
        new_rows = []
        for cc_label in cc_labels[1:]:
            
            new_row = [W, H, FPS, f0, f, oclass, table_boundary]

            cc_label_img = 1*(cc_img == cc_label).astype(&#39;uint8&#39;)
            active_px = np.argwhere(cc_label_img!=0)
            active_px = active_px[:,[1,0]]
            w0,h0,w,h = cv2.boundingRect(active_px)

            new_row += [w0, h0, w, h]

            new_rows += [new_row]

        return new_rows

    
    def _remove_outside_keyboard_detections(self, df, th=0.5):
        &#34;&#34;&#34; Removes all the keyboard detections that are less that are
        50% not inside the table boundary.

        Parameters
        ----------
        df : DataFrame
            DataFrame having keyboard detections with `roi-overlap-ratio`
            column.
        th : Float
            Detectons which are &lt; th are removed.
        &#34;&#34;&#34;
        for ridx, row in df.iterrows():
            if row[&#39;roi-overlap-ratio&#39;] &lt; th:
                df.drop([ridx], inplace = True)
        return df

    
    def _get_roi_overlap_ratio(self, hdf, table_boundary):
        &#34;&#34;&#34; Adds a column to keyboard detections, roi-overlap-ratio.
            
            - Table boundary = T
            - Keyboard detection = H
            - Overlap (O) = Intersection(T, H)
                overlap-ratio = Area(O) / Area(H)

        Parameters
        ----------
        hdf : DataFrame
            Dataframe having keyboard detections

        table_boundary : List[Int]
            Table boundary as list, [w0, h0, w, h]
        &#34;&#34;&#34;
        # Flag to turn off/on images
        show_images = False
        
        # Uncompressing boundary
        [tw0, th0, tw, th] = table_boundary
        
        # Creating an image with zeros
        W = hdf[&#39;W&#39;].unique().item()
        H = hdf[&#39;H&#39;].unique().item()
        zimg = np.zeros((H,W))

        # Creating a binary image with table boundary marked as 1s
        timg = zimg.copy()
        timg[th0 : th0 + th, tw0 : tw0 + tw] = 1

        # Loop over each keyboard detection
        o_area_ratio_lst = []
        for ridx, row in hdf.iterrows():

            # keyboard detection is loaded into proper variables
            [hw0, hh0, hw, hh] = [row[&#39;w0&#39;], row[&#39;h0&#39;], row[&#39;w&#39;], row[&#39;h&#39;]]
            h_area = hw*hh

            # Creating a binary image with keyboard detection as 1
            himg = zimg.copy()
            himg[hh0 : hh0 + hh, hw0 : hw0 + hw] = 1

            # Overlap image
            oimg = himg + timg
            oimg = 1*(oimg == 2)
            o_area = oimg.sum()

            if show_images:
                plt.subplot(221)
                ax1 = plt.subplot(2, 2, 1)
                ax2 = plt.subplot(2, 2, 2)
                ax3 = plt.subplot(2, 2, 3)
                ax1.imshow(timg)
                ax2.imshow(himg)
                ax3.imshow(oimg)
                plt.show()
                import pdb; pdb.set_trace()

            # Drop the keyboard detection if the overlap area is less than
            # 50% of keyboard area
            o_area_ratio = o_area/h_area
            o_area_ratio_lst += [o_area_ratio]
            
        hdf[&#39;table_boundary&#39;] = f&#34;{tw0}-{th0}-{tw}-{th}&#34;
        hdf[&#39;roi-overlap-ratio&#39;] = o_area_ratio_lst

        return hdf

    
    def _have_sufficient_rois(self, df):
        &#34;&#34;&#34; Returns true if there there is a vlaid region of interest of atleast one
        student. A student having atleast tow rois out of three is considered valid.

        Parameters
        ----------
        df : DataFrame
            A data frame having roi entries for `self.tydur`.
        &#34;&#34;&#34;

        # Dropping unnecessary columns
        df.drop([&#39;Time&#39;, &#39;f0&#39;, &#39;video_names&#39;, &#39;f&#39;], axis = 1, inplace=True)

        # Looping over each column and if atleast one column contain 2 valid entries
        # return true
        for col in df.columns.tolist():
            
            valid_bboxes = 0
            for i in range(0, self.tydur):

                # Bounding box in current column
                bbox_coords = df[col].iloc[i]

                # A bounding box is valid if it has four coordinates
                # separated by &#34;-&#34;
                if len(bbox_coords.split(&#34;-&#34;)) == 4:
                    valid_bboxes += 1

                # If we are able to get more than 2 valid bounding
                # boxes return True
                if valid_bboxes &gt;= 2:
                    return True

        # If thre are no valid bounding boxes return False
        return False

    
    def _get_table_boundary(self, df):
        &#34;&#34;&#34; Return table boundary as list, [w0, h0, w, h]

        Parameters
        ----------
        df : DataFrame
            A data frame having roi entries for `self.tydur`.
        &#34;&#34;&#34;

        # Dropping unnecessary columns
        df.drop([&#39;Time&#39;, &#39;f0&#39;, &#39;video_names&#39;, &#39;f&#39;], axis = 1, inplace=True)

        # Creating list of bounding box coordinates
        w_min = math.inf
        h_min = math.inf
        w_max = 0
        h_max = 0
        for col in df.columns.tolist():
            for ridx in range(0, len(df[col])):
                
                [w0, h0, w, h] = [int(x) for x in df[col].iloc[ridx].split(&#39;-&#39;)]

                if w_min &gt; w0:
                    w_min = w0
                if h_min &gt; h0:
                    h_min = h0
                if w_max &lt; w0 + w:
                    w_max = w0 + w
                if h_max &lt; h0 + h:
                    h_max = h0 + h

        boundary = [w_min, h_min, w_max - w_min, h_max - h_min]

        return boundary
        

    
    def _check_cfg(self, cfg):
        &#34;&#34;&#34; Validates the configuration file.&#34;&#34;&#34;

        # Check files
        pk.check_file_existance(cfg[&#39;video&#39;])
        pk.check_file_existance(cfg[&#39;keyboard&#39;])
        pk.check_file_existance(cfg[&#39;table_roi&#39;])
        pk.check_file_existance(cfg[&#39;ckpt&#39;])
        
        # Depth should be between 2 and 4
        if cfg[&#39;depth&#39;] &lt; 2 or cfg[&#39;depth&#39;] &gt; 4:
            raise Exception(
                &#34;USER EXCEPTION: The dyadic depth is not valid.&#34;
                f&#34;\n\t Depth: {cfg[&#39;depth&#39;]}&#34;
            )


        
    def _load_net(self, cfg):
        &#34;&#34;&#34; Load neural network to GPU. &#34;&#34;&#34;

        print(&#34;INFO: Loading Trained network to GPU ...&#34;)

        # Checkpoint and depth from cfg file.
        ckpt = cfg[&#39;ckpt&#39;]
        depth = cfg[&#39;depth&#39;]

        # Creating an instance of Dyadic 3D-CNN
        net = DyadicCNN3D(depth, [3, 90, 224, 224])
        net.to(&#34;cuda:0&#34;)

        # Print summary of network.
        # summary(net, (3, 90, 224, 224))

        # Loading the net with trained weights to cuda device 0
        ckpt_weights = torch.load(ckpt)
        net.load_state_dict(ckpt_weights[&#39;model_state_dict&#39;])
        net.eval()

        return net
        
        

    def _check_for_typing(self, proposal_df):
        &#34;&#34;&#34; Checks for typing in the proposal data frame.

        Parameters
        ----------
        proposal_df: DataFrame
            Proposal dataframe having keyboards bounding boxes.
        Todo
        ----
        1. Here I am trimming -&gt; typing to hdd -&gt; loading. This is not
           recommended for speed. Please try to improve.
        &#34;&#34;&#34;
        import pdb; pdb.set_trace()
        # Loop over proposal dataframe
        for i, row in proposal_df.iterrows():

            # if w or h == 0 then there is no keyboards
            if row[&#39;w&#39;] == 0 or row[&#39;h&#39;] == 0:
                proposal_df.at[i, &#39;typing&#39;] = -1
            else:
                # Creating temporal trim
                bbox = [row[&#39;w0&#39;],row[&#39;h0&#39;], row[&#39;w&#39;], row[&#39;h&#39;]]
                sfrm = row[&#39;f0&#39;]
                efrm = sfrm + row[&#39;f&#39;]
                oloc = f&#34;{os.path.dirname(self._vid.props[&#39;dir_loc&#39;])}&#34;
                opth = (f&#34;{oloc}/temp.mp4&#34;)

                # Spatio temporal trim
                self._vid.spatiotemporal_trim(sfrm, efrm, bbox, opth)
                 
                # Creating a temporary text file `temp.txt` having
                # temp.mp4 and a dummy label (100)
                with open(f&#34;{oloc}/temp.txt&#34;, &#34;w&#34;) as f:
                    f.write(&#34;temp.mp4 100&#34;)

                # Intialize AOLME data loader instance
                tst_data = AOLMETrmsDLoader(oloc, f&#34;{oloc}/temp.txt&#34;, oshape=(224, 224))
                tst_loader = DataLoader(tst_data, batch_size=1, num_workers=1)

                # Loop over tst data (goes over only once. I am desparate so I kept the loop)
                for data in tst_loader:
                    dummy_labels, inputs = (data[0].to(&#34;cuda:0&#34;, non_blocking=True),
                                             data[1].to(&#34;cuda:0&#34;, non_blocking=True))
                    with torch.no_grad():
                        outputs = self._net(inputs)
                        ipred = outputs.data.clone()
                        ipred = ipred.to(&#34;cpu&#34;).numpy().flatten().tolist()
                        proposal_df.at[i, &#39;typing&#39;] = round(ipred[0])
                        
        return proposal_df



    def _get_proposal_df(self, bboxes, tydur):
        &#34;&#34;&#34;
        OBSOLETE SHOULD BE DELETED IN CLEANUP PHASE.
        Creates a data frame with each row representing 3 seconds.

        Parameters
        ----------
        bboxes: str
            path to file having keyboards bounding boxes
        tydur: int, optional
            Each typing instance duration considered in seconds. 
            Defaults to 3.
        &#34;&#34;&#34;
        # Video properties
        num_frames = self._vid.props[&#39;num_frames&#39;]
        fps = self._vid.props[&#39;frame_rate&#39;]

        # Creating f0 and f lists
        num_trims = math.floor(num_frames/(tydur*fps))
        f0 = [x*(tydur*fps) for x in range(0, num_trims)]
        f = [tydur*fps]*num_trims

        # Creating W, H and FPS lists
        W = [self._vid.props[&#39;width&#39;]]*num_trims
        H = [self._vid.props[&#39;height&#39;]]*num_trims
        fps_lst = [fps]*num_trims

        # Get bounding boxes
        w0, h0, w, h = self._get_proposal_bboxes(bboxes, f0, f)

        # Intializing all typing instances are marked nan(numpy)
        typing_lst = [np.nan]*(num_trims)

        # Creating data frame with all the lists
        df = pd.DataFrame(list(zip(W, H, fps_lst, f0, f, w0, h0, w, h, typing_lst)),
                          columns=[&#34;W&#34;,&#34;H&#34;, &#34;FPS&#34;, &#34;f0&#34;, &#34;f&#34;, &#34;w0&#34;, &#34;h0&#34;, &#34;w&#34;, &#34;h&#34;, &#34;typing&#34;])
        return df


    
    def write_to_csv(self):
        &#34;&#34;&#34; Writes typing instances to a csv file. The name of the file is `&lt;video name&gt;_wr_using_alg.csv` and has
        following columns,

            1. f0      : poc of starting frame
            2. f       : number of frames
            3. W, H    : Video width and height
            4. w0, h0  : Bounding box top left corner
            5. w, h    : width and height of bounding box
            6. FPS     : Frames per second
            7. typing : {-1, 0, 1}.
                -1 =&gt; Keyboards not found
                0  =&gt; notyping
                1  =&gt; typing

        &#34;&#34;&#34;
        # Update typing instances in typing dataframe by processing
        # valid instances to 0 or 1
        self.tydf = self._check_for_typing(self._proposal_df.copy())
        
        vname = self._vid.props[&#39;name&#39;]
        vloc = self._vid.props[&#39;dir_loc&#39;]
        csv_pth = f&#34;{vloc}/{vname}_wr_using_alg.csv&#34;
        self.tydf.to_csv(csv_pth)
        

        
    def _get_proposal_bboxes(self, bboxes, f0_lst, f_lst):
        &#34;&#34;&#34; Creates proposal bounding boxes. Trims from these bounding boxes are
        later processed via typing detection algorithm.

        If there are multiple bounding boxes in the duration we consider the
        union of bounding boxes.

        Parameters
        ----------
        bboxes: str
            Path to file having keyboards detection bounding boxes
        f0_lst: List of int
            List having starting frame poc
        f_lst: List of int
            List having poc lenght
        &#34;&#34;&#34;
        df_bb = pd.read_csv(bboxes)
        num_trims = len(f0_lst)
        
        w0_lst = [0]*num_trims
        h0_lst = [0]*num_trims
        w_lst = [0]*num_trims
        h_lst = [0]*num_trims
        
        for i in range(0, num_trims):
            f0 = f0_lst[i]
            f = f_lst[i]

            # Data frame having detections from f0 to f0+f
            df_bbi = pd.DataFrame()
            df_bbi = df_bb[df_bb[&#39;f0&#39;] &gt;= f0].copy()
            df_bbi = df_bbi[df_bbi[&#39;f0&#39;] &lt; f0 + f]
            if len(df_bbi) &gt; 0:
                w0i = df_bbi[&#39;w0&#39;].tolist()
                h0i = df_bbi[&#39;h0&#39;].tolist()
                wi = df_bbi[&#39;w&#39;].tolist()
                hi = df_bbi[&#39;h&#39;].tolist()
                w1i = [sum(x) for x in zip(w0i, wi)]
                h1i = [sum(x) for x in zip(h0i, hi)]

                # Taking union
                w0_tl = min(w0i)
                h0_tl = min(h0i)
                w0_br = max(w1i)
                h0_br = max(h1i)

                # Adding to the list
                w0_lst[i] = w0_tl
                h0_lst[i] = h0_tl
                w_lst[i] = w0_br - w0_tl
                h_lst[i] = h0_br - h0_tl
        return (w0_lst, h0_lst, w_lst, h_lst)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="aqua.frameworks.typing.framework2.Typing.cfg"><code class="name">var <span class="ident">cfg</span></code></dt>
<dd>
<div class="desc"><p>Configuration dictionary</p></div>
</dd>
<dt id="aqua.frameworks.typing.framework2.Typing.ivid"><code class="name">var <span class="ident">ivid</span></code></dt>
<dd>
<div class="desc"><p>Input video instance.</p></div>
</dd>
<dt id="aqua.frameworks.typing.framework2.Typing.tydf"><code class="name">var <span class="ident">tydf</span></code></dt>
<dd>
<div class="desc"><p>Data frame having information on typing instances.
It has following columns,</p>
<pre><code>1. f0     : poc of starting frame
2. f      : number of frames
3. W, H   : Video width and height
4. w0, h0 : Bounding box top left corner
5. w, h   : width and height of bounding box
6. FPS    : Frames per second
7. typing : {-1, 0, 1}.
            -1 =&gt; Keyboard not found
             0 =&gt; notyping
             1 =&gt; typing
</code></pre></div>
</dd>
<dt id="aqua.frameworks.typing.framework2.Typing.tydur"><code class="name">var <span class="ident">tydur</span></code></dt>
<dd>
<div class="desc"><p>Duration of spati-temporal trims to classify.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="aqua.frameworks.typing.framework2.Typing.classify_typing_proposals"><code class="name flex">
<span>def <span class="ident">classify_typing_proposals</span></span>(<span>self, trp, debug=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Classify each proposed region as typing / no-typing.</p>
<h2 id="todo">Todo</h2>
<p>This function evaluates one proposal at a time. This is note
optimal. I have to redo this to evaluate multiple proposals
at a time. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>trp</code></strong> :&ensp;<code>Pandas DataFrame instance</code></dt>
<dd>DataFrame having typing region proposals</dd>
<dt><strong><code>debug</code></strong> :&ensp;<code>Bool</code></dt>
<dd>When True we will save the class probability and trimmed
videos.</dd>
</dl>
<h2 id="return">Return</h2>
<p>A DataFrame with <code>typing</code> column with labels 1 and 0 for
typing and no-typing respectively.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def classify_typing_proposals(self, trp, debug=True):
    &#34;&#34;&#34; Classify each proposed region as typing / no-typing.

    Todo
    ----
    This function evaluates one proposal at a time. This is note
    optimal. I have to redo this to evaluate multiple proposals
    at a time. 

    Parameters
    ----------
    trp : Pandas DataFrame instance
        DataFrame having typing region proposals
    debug : Bool
        When True we will save the class probability and trimmed
        videos.

    Return
    ------
    A DataFrame with `typing` column with labels 1 and 0 for
    typing and no-typing respectively.
    &#34;&#34;&#34;
    
    # Output location to save trims and typing csv file
    oloc = self.ivid.props[&#39;dir_loc&#39;]
    
    # Creating another column in typing region proposal dataframe
    trp[&#39;typing&#39;] = -1
    if debug:
        print(f&#34;INFO: RUNNING IN DEBUG MODE!&#34;)
        trp[&#39;p&#39;] = -1
        trp[&#39;trim_path&#39;] = &#34;&#34;
        os.system(f&#34;rm -r {oloc}/trims&#34;)
        os.system(f&#34;mkdir -p {oloc}/trims/0&#34;)
        os.system(f&#34;mkdir -p {oloc}/trims/1&#34;)
    
    # Loading neural network into GPU
    net = self._load_net(self.cfg)

    # Loop through typing proposal
    for i, row in tqdm(trp.iterrows(), total=trp.shape[0], desc=&#34;INFO: Classifying&#34;):

        # Spatio temporal trim coordinates
        bbox = [row[&#39;w0&#39;],row[&#39;h0&#39;], row[&#39;w&#39;], row[&#39;h&#39;]]
        sfrm = row[&#39;f0&#39;]
        efrm = sfrm + row[&#39;f&#39;]
        opth = (f&#34;{oloc}/temp.mp4&#34;)

        # Spatio temporal trim
        self.ivid.save_spatiotemporal_trim(sfrm, efrm, bbox, opth)

        # Creating a temporary text file `temp.txt` having
        # temp.mp4 and a dummy label (100)
        with open(f&#34;{oloc}/temp.txt&#34;, &#34;w&#34;) as f:
            f.write(&#34;temp.mp4 100&#34;)

                

        # Intialize AOLME data loader instance
        tst_data = AOLMETrmsDLoader(
            oloc, f&#34;{oloc}/temp.txt&#34;, oshape=(224, 224)
        )
        tst_loader = DataLoader(
            tst_data, batch_size=1, num_workers=1
        )

        # Loop over tst data (??? goes over only once. I am desparate so I kept the loop)
        for data in tst_loader:
            dummy_labels, inputs = (
                data[0].to(&#34;cuda:0&#34;, non_blocking=True),
                data[1].to(&#34;cuda:0&#34;, non_blocking=True)
            )
            with torch.no_grad():
                try:
                    outputs = net(inputs)
                except:
                    import pdb; pdb.set_trace()
                ipred = outputs.data.clone()
                ipred = ipred.to(&#34;cpu&#34;).numpy().flatten().tolist()
                trp.at[i, &#39;typing&#39;] = round(ipred[0])
                if debug:
                    trim_pth = f&#34;{oloc}/trims/{round(ipred[0])}/trim_{i}.mp4&#34;
                    os.system(f&#34;cp {oloc}/temp.mp4 {trim_pth}&#34;)
                    trp.at[i, &#39;p&#39;] = ipred[0]
                    trp.at[i, &#39;trim_path&#39;] = f&#34;{round(ipred[0])}/trim_{i}.mp4&#34;
    return trp</code></pre>
</details>
</dd>
<dt id="aqua.frameworks.typing.framework2.Typing.get_typing_proposals"><code class="name flex">
<span>def <span class="ident">get_typing_proposals</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates typing proposal regions by using keyboard detections
and manually annotated table region of interest.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_typing_proposals(self):
    &#34;&#34;&#34; Generates typing proposal regions by using keyboard detections
    and manually annotated table region of interest.
    &#34;&#34;&#34;
    
    # Creating an empty typing region proposal dataframe
    tyrp = pd.DataFrame()

    # Load keyboard and table roi csvs
    keyboard_df = pd.read_csv(self.cfg[&#39;keyboard&#39;])
    troi_df = pd.read_csv(self.cfg[&#39;table_roi&#39;])
    troi_df = troi_df[troi_df[&#39;video_names&#39;] == f&#34;{self.ivid.props[&#39;name&#39;]}{self.ivid.props[&#39;extension&#39;]}&#34;].copy()
    
    # Calcuating typing region proposals every 3 seconds using keyboard detections
    fps = keyboard_df[&#39;FPS&#39;].unique().item()
    new_rows = []
    for i in tqdm(range(0, troi_df[&#39;f0&#39;].max(), 3*fps), desc=&#34;INFO: typing proposals&#34;):

        # Get keyboards and table regions for 3 seconds
        kdf = keyboard_df[keyboard_df[&#39;f0&#39;].between(i, i + 3*fps - 1)].copy()
        tdf = troi_df[troi_df[&#39;f0&#39;].between(i, i + 3*fps - 1)].copy()

        # Skip,
        # 1. if the current 3 seconds do not have sufficient table regions of interest
        # 2. if the keyboards dataframe is empty
        if (not self._have_sufficient_rois(tdf.copy())) or (len(kdf) == 0):
            continue

        # Calculating table boundary from regions of interest
        table_boundary = self._get_table_boundary(tdf.copy())

        # Calculating overlap ratio between table bondary and keyboard detections
        kdf = self._get_roi_overlap_ratio(kdf, table_boundary)

        # Remove keyboard regions that do not corss a overlap ratio threshold
        kdf = self._remove_outside_keyboard_detections(kdf.copy())

        # Skip to next 3 seconds if all the keyboard regions lie outside
        # the table boundary
        if len(kdf) == 0:
            continue

        # Calculate keyboard region proposal rows for 3 seconds by tabking union
        # of all the valid keyboard detections
        new_rows += self._get_union_of_keyboard_detections(kdf.copy(), i, 3*fps)


    tyrp = pd.DataFrame(new_rows, columns=[
            &#39;W&#39;, &#39;H&#39;, &#39;FPS&#39;, &#39;f0&#39;, &#39;f&#39;, &#39;class&#39;, &#39;table_boundary&#39;,
            &#39;w0&#39;, &#39;h0&#39;, &#39;w&#39;, &#39;h&#39;
        ])
        
    return tyrp</code></pre>
</details>
</dd>
<dt id="aqua.frameworks.typing.framework2.Typing.write_to_csv"><code class="name flex">
<span>def <span class="ident">write_to_csv</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Writes typing instances to a csv file. The name of the file is <code>&lt;video name&gt;_wr_using_alg.csv</code> and has
following columns,</p>
<pre><code>1. f0      : poc of starting frame
2. f       : number of frames
3. W, H    : Video width and height
4. w0, h0  : Bounding box top left corner
5. w, h    : width and height of bounding box
6. FPS     : Frames per second
7. typing : {-1, 0, 1}.
    -1 =&gt; Keyboards not found
    0  =&gt; notyping
    1  =&gt; typing
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_to_csv(self):
    &#34;&#34;&#34; Writes typing instances to a csv file. The name of the file is `&lt;video name&gt;_wr_using_alg.csv` and has
    following columns,

        1. f0      : poc of starting frame
        2. f       : number of frames
        3. W, H    : Video width and height
        4. w0, h0  : Bounding box top left corner
        5. w, h    : width and height of bounding box
        6. FPS     : Frames per second
        7. typing : {-1, 0, 1}.
            -1 =&gt; Keyboards not found
            0  =&gt; notyping
            1  =&gt; typing

    &#34;&#34;&#34;
    # Update typing instances in typing dataframe by processing
    # valid instances to 0 or 1
    self.tydf = self._check_for_typing(self._proposal_df.copy())
    
    vname = self._vid.props[&#39;name&#39;]
    vloc = self._vid.props[&#39;dir_loc&#39;]
    csv_pth = f&#34;{vloc}/{vname}_wr_using_alg.csv&#34;
    self.tydf.to_csv(csv_pth)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="aqua.frameworks.typing" href="index.html">aqua.frameworks.typing</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="aqua.frameworks.typing.framework2.Typing" href="#aqua.frameworks.typing.framework2.Typing">Typing</a></code></h4>
<ul class="">
<li><code><a title="aqua.frameworks.typing.framework2.Typing.cfg" href="#aqua.frameworks.typing.framework2.Typing.cfg">cfg</a></code></li>
<li><code><a title="aqua.frameworks.typing.framework2.Typing.classify_typing_proposals" href="#aqua.frameworks.typing.framework2.Typing.classify_typing_proposals">classify_typing_proposals</a></code></li>
<li><code><a title="aqua.frameworks.typing.framework2.Typing.get_typing_proposals" href="#aqua.frameworks.typing.framework2.Typing.get_typing_proposals">get_typing_proposals</a></code></li>
<li><code><a title="aqua.frameworks.typing.framework2.Typing.ivid" href="#aqua.frameworks.typing.framework2.Typing.ivid">ivid</a></code></li>
<li><code><a title="aqua.frameworks.typing.framework2.Typing.tydf" href="#aqua.frameworks.typing.framework2.Typing.tydf">tydf</a></code></li>
<li><code><a title="aqua.frameworks.typing.framework2.Typing.tydur" href="#aqua.frameworks.typing.framework2.Typing.tydur">tydur</a></code></li>
<li><code><a title="aqua.frameworks.typing.framework2.Typing.write_to_csv" href="#aqua.frameworks.typing.framework2.Typing.write_to_csv">write_to_csv</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>