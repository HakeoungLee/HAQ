<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>aqua.objdets.projections API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>aqua.objdets.projections</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import pdb
import sys
import cv2
import skimage
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from aqua.video_tools import Vid
from scipy import ndimage as ndi
from skimage.segmentation import watershed
from skimage.feature import peak_local_max


class ObjDetProjs:
    def __init__(self, bboxes_csv, proj_interval):
        &#34;&#34;&#34; Creates heat maps using bounding boxes.
        The csv file having bounding boxes is expedted to have following
        columns,
        `{f0, class, W, H, w0, h0, w, h}`.

        It removes bounding boxes which are very small. The threshold
        is dynamically determined per video. Bounding boxes less than
        Q1(25 percentile) size `(w * h)` are removed.

        Parameters
        ----------
        bboxes_csv: str
            Path to csv file having bounding boxes with required columns.
        proj_interval: int
            Projections are generated ever &#39;proj_interval&#39; seconds. A value
            of -1 assumes that we are projecting for entire video
        &#34;&#34;&#34;
        # Load file
        if not os.path.isfile(bboxes_csv):
            raise Exception(f&#34;{bboxes_csv} does not exist&#34;)
        self._bboxes_df = pd.read_csv(bboxes_csv)

        self._t = proj_interval

    def display_on_video(self, vipth, ws=False):
        &#34;&#34;&#34; Saves projections to video

        Parameters
        ----------
        vipth: str
            Input video path
        ws: bool, optional
            Applies water shed when true. Defaults to `False`.
        &#34;&#34;&#34;
        vin = Vid(vipth)
        nfrms = vin.props[&#39;num_frames&#39;]

        # Loop over projection intervals
        if self._t &lt; 0:
            frms_to_skip = nfrms
        else:
            frms_to_skip = self._t * vin.props[&#39;frame_rate&#39;]
        for spoc in range(0, nfrms, frms_to_skip):
            epoc = spoc + frms_to_skip  # end

            cur_bboxes_df = self._bboxes_df[
                self._bboxes_df[&#39;f0&#39;] &gt; spoc].copy()
            cur_bboxes_df = cur_bboxes_df[cur_bboxes_df[&#39;f0&#39;] &lt; epoc]

            # Only if the detections are presend do these
            if len(cur_bboxes_df) &gt; 0:
                proj_img = self._create_proj_map(cur_bboxes_df.copy())
                if ws:
                    proj_rgb = 255 * (
                        self._apply_watershed(proj_img &gt; 0))
                else:
                    proj_img_scaled = 255*(proj_img)
                    proj_rgb = np.zeros((vin.props[&#39;height&#39;], vin.props[&#39;width&#39;], 3))
                    proj_rgb[:,:,0] = proj_img_scaled
                    proj_rgb[:,:,1] = proj_img_scaled
                    proj_rgb[:,:,2] = proj_img_scaled


                # Get the middle frame
                # Video loop
                
                for poc in range(spoc, epoc, vin.props[&#39;frame_rate&#39;]):
                    frm = vin.get_frame(poc)

                    # Blend images
                    alpha = 0.3
                    beta = 1 - alpha
                    blended_frm = cv2.addWeighted(
                        frm, alpha, proj_rgb.astype(&#39;uint8&#39;), beta, 0.0)
                    cv2.imshow(&#34;Blended(q to quit, p to pause)&#34;, blended_frm)
                    cv2.imshow(
                        &#34;Projection map(q to quit, p to pause)&#34;, proj_img)
                    key = cv2.waitKey(33)
                    if key == ord(&#39;q&#39;):
                        break
                    if key == ord(&#39;p&#39;):
                        cv2.waitKey(-1)  # wait until any key is pressed

    def _apply_watershed(self, image):
        &#34;&#34;&#34; Applies watershed to gray scale image.

        image: ndarray
            A gray scale image.
        &#34;&#34;&#34;
        image = image.astype(&#34;uint8&#34;).copy()
        distance = ndi.distance_transform_edt(image)
        coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=image)
        mask = np.zeros(distance.shape, dtype=bool)
        mask[tuple(coords.T)] = True
        markers, _ = ndi.label(mask)
        labels = watershed(-distance, markers, mask=image)
        labels_rgb = skimage.color.label2rgb(labels, bg_label=0, bg_color=(0,0,0))
        return 255*labels_rgb


    def write_to_video(self, vipth, vopth):
        &#34;&#34;&#34; Saves projections to video

         Parameters
         ----------
         vipth: Input video path
         vopth: Output path
         &#34;&#34;&#34;
        vin = Vid(vipth)
        nfrms = vin.props[&#39;num_frames&#39;]

        # output writer
        vout = cv2.VideoWriter(
            vopth,
            cv2.VideoWriter_fourcc(&#39;M&#39;,&#39;J&#39;,&#39;P&#39;,&#39;G&#39;), 30,
            (vin.props[&#39;width&#39;],vin.props[&#39;height&#39;])
        )

        # Loop over projection intervals
        if self._t &lt; 0:
            frms_to_skip = nfrms
        else:
            frms_to_skip = self._t*vin.props[&#39;frame_rate&#39;]
        for spoc in range(0, nfrms, frms_to_skip):
            mpoc = spoc + (frms_to_skip/2) # middle
            epoc = min(spoc + frms_to_skip, nfrms-1) # end

            cur_bboxes_df = self._bboxes_df[self._bboxes_df[&#39;f0&#39;] &gt; spoc].copy()
            cur_bboxes_df = cur_bboxes_df[cur_bboxes_df[&#39;f0&#39;] &lt; epoc]

            # Only if the detections are presend do these
            if len(cur_bboxes_df) &gt; 0:
                proj_img = self._create_proj_map(cur_bboxes_df.copy())
                proj_img_scaled = 255*(proj_img)
                proj_rgb = np.zeros((vin.props[&#39;height&#39;], vin.props[&#39;width&#39;], 3))
                proj_rgb[:,:,0] = proj_img_scaled
                proj_rgb[:,:,1] = proj_img_scaled
                proj_rgb[:,:,2] = proj_img_scaled


                # Get the middle frame
                # Video loop
                for poc in range(spoc, epoc, vin.props[&#39;frame_rate&#39;]):
                    print(poc)
                    frm = vin.get_frame(poc)

                    # Blend images
                    alpha = 0.3
                    beta = 1 - alpha
                    blended_frm = cv2.addWeighted(frm, alpha, proj_rgb.astype(&#39;uint8&#39;), beta, 0.0)
                    vout.write(blended_frm)
        vout.release()






    def get_proj_map_for_full_video(self):
        &#34;&#34;&#34;
        Returns projection map as numpy array for complete video
        &#34;&#34;&#34;
        full_video_proj = self._create_proj_map(self._bboxes_df.copy())
        return full_video_proj

    def _create_proj_map(self, df_bboxes):
        &#34;&#34;&#34; Creates a projection for full video as numpy
        array

        Parameters
        ----------
        df_bboxes: DataFrame
            DataFrame having bounding box values
        &#34;&#34;&#34;
        try:
            W = df_bboxes[&#39;W&#39;].unique().item()
        except:
            pdb.set_trace()
        H = df_bboxes[&#39;H&#39;].unique().item()

        # Initializing numpy array to zeros
        hm_np = np.zeros((H,W))

        # Bboxes loop
        for idx, row in df_bboxes.iterrows():

            # Current bounding box information
            w = row[&#39;w&#39;]
            h = row[&#39;h&#39;]
            w0 = row[&#39;w0&#39;]
            h0 = row[&#39;h0&#39;]

            # Creating binary image for current bounding box
            tmp_bimg = np.zeros((H,W))
            tmp_bimg[h0:h0+h, w0:w0+h] = 1

            # Keep adding these images
            hm_np = hm_np + tmp_bimg

        # Normalize heat map to 0 and 1
        hm_np = hm_np/hm_np.max()
        return hm_np</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="aqua.objdets.projections.ObjDetProjs"><code class="flex name class">
<span>class <span class="ident">ObjDetProjs</span></span>
<span>(</span><span>bboxes_csv, proj_interval)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates heat maps using bounding boxes.
The csv file having bounding boxes is expedted to have following
columns,
<code>{f0, class, W, H, w0, h0, w, h}</code>.</p>
<p>It removes bounding boxes which are very small. The threshold
is dynamically determined per video. Bounding boxes less than
Q1(25 percentile) size <code>(w * h)</code> are removed.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>bboxes_csv</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to csv file having bounding boxes with required columns.</dd>
<dt><strong><code>proj_interval</code></strong> :&ensp;<code>int</code></dt>
<dd>Projections are generated ever 'proj_interval' seconds. A value
of -1 assumes that we are projecting for entire video</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ObjDetProjs:
    def __init__(self, bboxes_csv, proj_interval):
        &#34;&#34;&#34; Creates heat maps using bounding boxes.
        The csv file having bounding boxes is expedted to have following
        columns,
        `{f0, class, W, H, w0, h0, w, h}`.

        It removes bounding boxes which are very small. The threshold
        is dynamically determined per video. Bounding boxes less than
        Q1(25 percentile) size `(w * h)` are removed.

        Parameters
        ----------
        bboxes_csv: str
            Path to csv file having bounding boxes with required columns.
        proj_interval: int
            Projections are generated ever &#39;proj_interval&#39; seconds. A value
            of -1 assumes that we are projecting for entire video
        &#34;&#34;&#34;
        # Load file
        if not os.path.isfile(bboxes_csv):
            raise Exception(f&#34;{bboxes_csv} does not exist&#34;)
        self._bboxes_df = pd.read_csv(bboxes_csv)

        self._t = proj_interval

    def display_on_video(self, vipth, ws=False):
        &#34;&#34;&#34; Saves projections to video

        Parameters
        ----------
        vipth: str
            Input video path
        ws: bool, optional
            Applies water shed when true. Defaults to `False`.
        &#34;&#34;&#34;
        vin = Vid(vipth)
        nfrms = vin.props[&#39;num_frames&#39;]

        # Loop over projection intervals
        if self._t &lt; 0:
            frms_to_skip = nfrms
        else:
            frms_to_skip = self._t * vin.props[&#39;frame_rate&#39;]
        for spoc in range(0, nfrms, frms_to_skip):
            epoc = spoc + frms_to_skip  # end

            cur_bboxes_df = self._bboxes_df[
                self._bboxes_df[&#39;f0&#39;] &gt; spoc].copy()
            cur_bboxes_df = cur_bboxes_df[cur_bboxes_df[&#39;f0&#39;] &lt; epoc]

            # Only if the detections are presend do these
            if len(cur_bboxes_df) &gt; 0:
                proj_img = self._create_proj_map(cur_bboxes_df.copy())
                if ws:
                    proj_rgb = 255 * (
                        self._apply_watershed(proj_img &gt; 0))
                else:
                    proj_img_scaled = 255*(proj_img)
                    proj_rgb = np.zeros((vin.props[&#39;height&#39;], vin.props[&#39;width&#39;], 3))
                    proj_rgb[:,:,0] = proj_img_scaled
                    proj_rgb[:,:,1] = proj_img_scaled
                    proj_rgb[:,:,2] = proj_img_scaled


                # Get the middle frame
                # Video loop
                
                for poc in range(spoc, epoc, vin.props[&#39;frame_rate&#39;]):
                    frm = vin.get_frame(poc)

                    # Blend images
                    alpha = 0.3
                    beta = 1 - alpha
                    blended_frm = cv2.addWeighted(
                        frm, alpha, proj_rgb.astype(&#39;uint8&#39;), beta, 0.0)
                    cv2.imshow(&#34;Blended(q to quit, p to pause)&#34;, blended_frm)
                    cv2.imshow(
                        &#34;Projection map(q to quit, p to pause)&#34;, proj_img)
                    key = cv2.waitKey(33)
                    if key == ord(&#39;q&#39;):
                        break
                    if key == ord(&#39;p&#39;):
                        cv2.waitKey(-1)  # wait until any key is pressed

    def _apply_watershed(self, image):
        &#34;&#34;&#34; Applies watershed to gray scale image.

        image: ndarray
            A gray scale image.
        &#34;&#34;&#34;
        image = image.astype(&#34;uint8&#34;).copy()
        distance = ndi.distance_transform_edt(image)
        coords = peak_local_max(distance, footprint=np.ones((3, 3)), labels=image)
        mask = np.zeros(distance.shape, dtype=bool)
        mask[tuple(coords.T)] = True
        markers, _ = ndi.label(mask)
        labels = watershed(-distance, markers, mask=image)
        labels_rgb = skimage.color.label2rgb(labels, bg_label=0, bg_color=(0,0,0))
        return 255*labels_rgb


    def write_to_video(self, vipth, vopth):
        &#34;&#34;&#34; Saves projections to video

         Parameters
         ----------
         vipth: Input video path
         vopth: Output path
         &#34;&#34;&#34;
        vin = Vid(vipth)
        nfrms = vin.props[&#39;num_frames&#39;]

        # output writer
        vout = cv2.VideoWriter(
            vopth,
            cv2.VideoWriter_fourcc(&#39;M&#39;,&#39;J&#39;,&#39;P&#39;,&#39;G&#39;), 30,
            (vin.props[&#39;width&#39;],vin.props[&#39;height&#39;])
        )

        # Loop over projection intervals
        if self._t &lt; 0:
            frms_to_skip = nfrms
        else:
            frms_to_skip = self._t*vin.props[&#39;frame_rate&#39;]
        for spoc in range(0, nfrms, frms_to_skip):
            mpoc = spoc + (frms_to_skip/2) # middle
            epoc = min(spoc + frms_to_skip, nfrms-1) # end

            cur_bboxes_df = self._bboxes_df[self._bboxes_df[&#39;f0&#39;] &gt; spoc].copy()
            cur_bboxes_df = cur_bboxes_df[cur_bboxes_df[&#39;f0&#39;] &lt; epoc]

            # Only if the detections are presend do these
            if len(cur_bboxes_df) &gt; 0:
                proj_img = self._create_proj_map(cur_bboxes_df.copy())
                proj_img_scaled = 255*(proj_img)
                proj_rgb = np.zeros((vin.props[&#39;height&#39;], vin.props[&#39;width&#39;], 3))
                proj_rgb[:,:,0] = proj_img_scaled
                proj_rgb[:,:,1] = proj_img_scaled
                proj_rgb[:,:,2] = proj_img_scaled


                # Get the middle frame
                # Video loop
                for poc in range(spoc, epoc, vin.props[&#39;frame_rate&#39;]):
                    print(poc)
                    frm = vin.get_frame(poc)

                    # Blend images
                    alpha = 0.3
                    beta = 1 - alpha
                    blended_frm = cv2.addWeighted(frm, alpha, proj_rgb.astype(&#39;uint8&#39;), beta, 0.0)
                    vout.write(blended_frm)
        vout.release()






    def get_proj_map_for_full_video(self):
        &#34;&#34;&#34;
        Returns projection map as numpy array for complete video
        &#34;&#34;&#34;
        full_video_proj = self._create_proj_map(self._bboxes_df.copy())
        return full_video_proj

    def _create_proj_map(self, df_bboxes):
        &#34;&#34;&#34; Creates a projection for full video as numpy
        array

        Parameters
        ----------
        df_bboxes: DataFrame
            DataFrame having bounding box values
        &#34;&#34;&#34;
        try:
            W = df_bboxes[&#39;W&#39;].unique().item()
        except:
            pdb.set_trace()
        H = df_bboxes[&#39;H&#39;].unique().item()

        # Initializing numpy array to zeros
        hm_np = np.zeros((H,W))

        # Bboxes loop
        for idx, row in df_bboxes.iterrows():

            # Current bounding box information
            w = row[&#39;w&#39;]
            h = row[&#39;h&#39;]
            w0 = row[&#39;w0&#39;]
            h0 = row[&#39;h0&#39;]

            # Creating binary image for current bounding box
            tmp_bimg = np.zeros((H,W))
            tmp_bimg[h0:h0+h, w0:w0+h] = 1

            # Keep adding these images
            hm_np = hm_np + tmp_bimg

        # Normalize heat map to 0 and 1
        hm_np = hm_np/hm_np.max()
        return hm_np</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="aqua.objdets.projections.ObjDetProjs.display_on_video"><code class="name flex">
<span>def <span class="ident">display_on_video</span></span>(<span>self, vipth, ws=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves projections to video</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>vipth</code></strong> :&ensp;<code>str</code></dt>
<dd>Input video path</dd>
<dt><strong><code>ws</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Applies water shed when true. Defaults to <code>False</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def display_on_video(self, vipth, ws=False):
    &#34;&#34;&#34; Saves projections to video

    Parameters
    ----------
    vipth: str
        Input video path
    ws: bool, optional
        Applies water shed when true. Defaults to `False`.
    &#34;&#34;&#34;
    vin = Vid(vipth)
    nfrms = vin.props[&#39;num_frames&#39;]

    # Loop over projection intervals
    if self._t &lt; 0:
        frms_to_skip = nfrms
    else:
        frms_to_skip = self._t * vin.props[&#39;frame_rate&#39;]
    for spoc in range(0, nfrms, frms_to_skip):
        epoc = spoc + frms_to_skip  # end

        cur_bboxes_df = self._bboxes_df[
            self._bboxes_df[&#39;f0&#39;] &gt; spoc].copy()
        cur_bboxes_df = cur_bboxes_df[cur_bboxes_df[&#39;f0&#39;] &lt; epoc]

        # Only if the detections are presend do these
        if len(cur_bboxes_df) &gt; 0:
            proj_img = self._create_proj_map(cur_bboxes_df.copy())
            if ws:
                proj_rgb = 255 * (
                    self._apply_watershed(proj_img &gt; 0))
            else:
                proj_img_scaled = 255*(proj_img)
                proj_rgb = np.zeros((vin.props[&#39;height&#39;], vin.props[&#39;width&#39;], 3))
                proj_rgb[:,:,0] = proj_img_scaled
                proj_rgb[:,:,1] = proj_img_scaled
                proj_rgb[:,:,2] = proj_img_scaled


            # Get the middle frame
            # Video loop
            
            for poc in range(spoc, epoc, vin.props[&#39;frame_rate&#39;]):
                frm = vin.get_frame(poc)

                # Blend images
                alpha = 0.3
                beta = 1 - alpha
                blended_frm = cv2.addWeighted(
                    frm, alpha, proj_rgb.astype(&#39;uint8&#39;), beta, 0.0)
                cv2.imshow(&#34;Blended(q to quit, p to pause)&#34;, blended_frm)
                cv2.imshow(
                    &#34;Projection map(q to quit, p to pause)&#34;, proj_img)
                key = cv2.waitKey(33)
                if key == ord(&#39;q&#39;):
                    break
                if key == ord(&#39;p&#39;):
                    cv2.waitKey(-1)  # wait until any key is pressed</code></pre>
</details>
</dd>
<dt id="aqua.objdets.projections.ObjDetProjs.get_proj_map_for_full_video"><code class="name flex">
<span>def <span class="ident">get_proj_map_for_full_video</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns projection map as numpy array for complete video</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_proj_map_for_full_video(self):
    &#34;&#34;&#34;
    Returns projection map as numpy array for complete video
    &#34;&#34;&#34;
    full_video_proj = self._create_proj_map(self._bboxes_df.copy())
    return full_video_proj</code></pre>
</details>
</dd>
<dt id="aqua.objdets.projections.ObjDetProjs.write_to_video"><code class="name flex">
<span>def <span class="ident">write_to_video</span></span>(<span>self, vipth, vopth)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves projections to video</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>vipth</code></strong> :&ensp;<code>Input video path</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>vopth</code></strong> :&ensp;<code>Output path</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_to_video(self, vipth, vopth):
    &#34;&#34;&#34; Saves projections to video

     Parameters
     ----------
     vipth: Input video path
     vopth: Output path
     &#34;&#34;&#34;
    vin = Vid(vipth)
    nfrms = vin.props[&#39;num_frames&#39;]

    # output writer
    vout = cv2.VideoWriter(
        vopth,
        cv2.VideoWriter_fourcc(&#39;M&#39;,&#39;J&#39;,&#39;P&#39;,&#39;G&#39;), 30,
        (vin.props[&#39;width&#39;],vin.props[&#39;height&#39;])
    )

    # Loop over projection intervals
    if self._t &lt; 0:
        frms_to_skip = nfrms
    else:
        frms_to_skip = self._t*vin.props[&#39;frame_rate&#39;]
    for spoc in range(0, nfrms, frms_to_skip):
        mpoc = spoc + (frms_to_skip/2) # middle
        epoc = min(spoc + frms_to_skip, nfrms-1) # end

        cur_bboxes_df = self._bboxes_df[self._bboxes_df[&#39;f0&#39;] &gt; spoc].copy()
        cur_bboxes_df = cur_bboxes_df[cur_bboxes_df[&#39;f0&#39;] &lt; epoc]

        # Only if the detections are presend do these
        if len(cur_bboxes_df) &gt; 0:
            proj_img = self._create_proj_map(cur_bboxes_df.copy())
            proj_img_scaled = 255*(proj_img)
            proj_rgb = np.zeros((vin.props[&#39;height&#39;], vin.props[&#39;width&#39;], 3))
            proj_rgb[:,:,0] = proj_img_scaled
            proj_rgb[:,:,1] = proj_img_scaled
            proj_rgb[:,:,2] = proj_img_scaled


            # Get the middle frame
            # Video loop
            for poc in range(spoc, epoc, vin.props[&#39;frame_rate&#39;]):
                print(poc)
                frm = vin.get_frame(poc)

                # Blend images
                alpha = 0.3
                beta = 1 - alpha
                blended_frm = cv2.addWeighted(frm, alpha, proj_rgb.astype(&#39;uint8&#39;), beta, 0.0)
                vout.write(blended_frm)
    vout.release()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="aqua.objdets" href="index.html">aqua.objdets</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="aqua.objdets.projections.ObjDetProjs" href="#aqua.objdets.projections.ObjDetProjs">ObjDetProjs</a></code></h4>
<ul class="">
<li><code><a title="aqua.objdets.projections.ObjDetProjs.display_on_video" href="#aqua.objdets.projections.ObjDetProjs.display_on_video">display_on_video</a></code></li>
<li><code><a title="aqua.objdets.projections.ObjDetProjs.get_proj_map_for_full_video" href="#aqua.objdets.projections.ObjDetProjs.get_proj_map_for_full_video">get_proj_map_for_full_video</a></code></li>
<li><code><a title="aqua.objdets.projections.ObjDetProjs.write_to_video" href="#aqua.objdets.projections.ObjDetProjs.write_to_video">write_to_video</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>